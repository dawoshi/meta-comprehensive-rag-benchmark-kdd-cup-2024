## [meta-comprehensive-rag-benchmark-kdd-cup-2024](https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024)


<p align="center">
  <img width="100" src="https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png" />
  <img width="100" src="https://upload.wikimedia.org/wikipedia/en/7/7d/Bazel_logo.svg" />
  <img width="100" src = "https://upload.wikimedia.org/wikipedia/commons/1/18/ISO_C%2B%2B_Logo.svg" />
</p>


### 1ã€ä»£ç ç»“æ„

```text
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ bm25_retriever.py
â”œâ”€â”€ build.sh
â”œâ”€â”€ config.py
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ result.json
â”‚Â Â  â”œâ”€â”€ test_question.json
â”‚Â Â  â””â”€â”€ train_a.pdf
â”œâ”€â”€ faiss_retriever.py
â”œâ”€â”€ vllm_model.py
â”œâ”€â”€ pdf_parse.py
â”œâ”€â”€ pre_train_model
â”‚Â Â  â”œâ”€â”€ Qwen-7B-Chat
â”‚Â Â  â”‚Â Â  â””â”€â”€ download.py
â”‚Â Â  â”œâ”€â”€ bge-reranker-large
â”‚Â Â  â””â”€â”€ m3e-large
â”œâ”€â”€ qwen_generation_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ rerank_model.py
â”œâ”€â”€ run.py
â”œâ”€â”€ run.sh
â””â”€â”€ vllm_wrapper.py
```

### 2ã€[Introduction](https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024)
#### 2.1 This challenge comprises of three tasks designed to improve question-answering (QA) systems.

TASK #1: WEB-BASED RETRIEVAL SUMMARIZATION Participants receive 5 web pages per question, potentially containing relevant information. The objective is to measure the systems' capability to identify and condense this information into accurate answers.

TASK #2: KNOWLEDGE GRAPH AND WEB AUGMENTATION This task introduces mock APIs to access information from underlying mock Knowledge Graphs (KGs), with structured data possibly related to the questions. Participants use mock APIs, inputting parameters derived from the questions, to retrieve relevant data for answer formulation. The evaluation focuses on the systems' ability to query structured data and integrate information from various sources into comprehensive answers.

TASK #3: END-TO-END RAG The third task increases complexity by providing 50 web pages and mock API access for each question, encountering both relevant information and noises. It assesses the systems' skill in selecting the most important data from a larger set, reflecting the challenges of real-world information retrieval and integration.



query: how many 3-point attempts did steve nash average per game in seasons he made the 50-40-90 club?
answer: 4 3-points attempts per game

query: where did  the ceo of salesforce previously work?
answer: marc benioff spent 13 years at oracle, before launching   salesforce.


#### 2.2  CRAG Dataset Description

#### ğŸ“ QUESTION ANSWER PAIRS

CRAG includes question-answer pairs that mirror real scenarios. It covers five domains: Finance, Sports, Music, Movies, and Encyclopedia Open domain. These domains represent the spectrum of information change ratesâ€”rapid (Finance and Sports), gradual (Music and Movies), and stable (Open domain).

CRAG includes eight types of questions in English:

Simple question: Questions asking for simple facts, such as the birth date of a person and the authors of a book.
Simple question with some condition: Questions asking for simple facts with some given conditions, such as stock price on a certain date and a director's recent movies in a certain genre.
Set question Questions that expect a set of entities or objects as the answer. An example is what are the continents in the southern hemisphere?
Comparison question: Questions that may compare two entities, such as who started performing earlier, Adele or Ed Sheeran?
Aggregation question: Questions that may need aggregation of retrieval results to answer, for example, how many Oscar awards did Meryl Streep win?
Multi-hop questions: Questions that may require chaining multiple pieces of information to compose the answer, such as who acted in Ang Lee's latest movie?
Post-processing question: Questions that need reasoning or processing of the retrieved information to obtain the answer, for instance, How many days did Thurgood Marshall serve as a Supreme Court justice?
False Premise question: Questions that have a false preposition or assumption; for example, What's the name of Taylor Swift's rap album before she transitioned to pop? (Taylor Swift didn't release any rap album.)

#### ğŸ“ RETRIEVAL CONTENTS

The dataset includes web search results and mock KGs to mimic real-world RAG retrieval sources. Web search contents were created by storing up to 50 pages from search queries related to each question. Mock KGs were created using the data behind the questions, supplemented with "hard negative" data to simulate a more challenging retrieval environment. Mock APIs facilitate structured searches within these KGs, and we provide the same API for all five domains to simulate Knowledge Graph access.

#### [CRAG Dataset Documentation](https://gitlab.aicrowd.com/aicrowd/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024/meta-comphrehensive-rag-benchmark-starter-kit/-/blob/master/docs/dataset.md)

### 3ã€Solution

#### 3.1 Web page parser

##### 3.1.1 pdfåˆ†å—è§£æ
![åˆ†å—è§£æç¤ºä¾‹å›¾](images/html.jpeg)
å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å¸Œæœ›pdfè§£æèƒ½å°½å¯èƒ½çš„æŒ‰ç…§å¿«çŠ¶è¿›è¡Œè§£æï¼Œæ¯ä¸€å—å½“åšä¸€ä¸ªæ ·æœ¬ï¼Œè¿™æ ·èƒ½å°½å¯èƒ½çš„ä¿è¯pdfä¸­æ–‡æœ¬å†…å®¹çš„å®Œæ•´æ€§
æ”¹è¿›==ã€‹å¸Œæœ›å€ŸåŠ©OCRè¿›è¡Œpdfçš„å—çŠ¶è¯†åˆ«

##### 3.1.2 pdf æ»‘çª—æ³•è§£æ
![æ»‘çª—æ³•è§£æç¤ºä¾‹å›¾1](images/02.png)
![æ»‘çª—æ³•è§£æç¤ºä¾‹å›¾2](images/03.png)
å¦‚å›¾1,2 æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å›¾1å’Œå›¾2ä¸Šä¸‹æ–‡æ˜¯è¿ç»­çš„ï¼Œå¦‚ä½•ä¿è¯æ–‡æœ¬å†…å®¹çš„è·¨é¡µè¿ç»­æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºæ»‘çª—æ³•ã€‚
å…·ä½“çš„æŠŠpdfä¸­æ‰€æœ‰å†…å®¹å½“åšä¸€ä¸ªå­—ç¬¦ä¸²æ¥å¤„ç†ï¼ŒæŒ‰ç…§å¥å·è¿›è¡Œåˆ†å‰²ï¼Œæ ¹æ®åˆ†å‰²åçš„æ•°ç»„è¿›è¡Œæ»‘çª—ã€‚å…·ä½“çš„å¦‚ä¸‹æ‰€ç¤º:

["aa","bb","cc","dd"]

å¦‚æœå­—ç¬¦ä¸²é•¿åº¦ä¸º4, ç»è¿‡æ»‘çª—åçš„ç»“æœå¦‚ä¸‹:

aabb

bbcc

ccdd

æˆ‘ä»¬å¸Œæœ›æ»‘çª—æ³•åƒå·ç§¯ä¸€æ ·å¯ä»¥ä¸åŒçš„kernel,Stride,æ¥å¯»æ‰¾èƒ½è¦†ç›–åˆ°çš„æœ€ä¼˜çš„æ ·æœ¬å¬å›

#### 3.2 å¬å›

å¬å›ä¸»è¦ä½¿ç”¨langchainä¸­çš„retrieversè¿›è¡Œæ–‡æœ¬çš„å¬å›ã€‚æˆ‘ä»¬çŸ¥é“å‘é‡å¬å›å’Œbm25å¬å›å…·æœ‰äº’è¡¥æ€§ï¼Œå› æ­¤é€‰ç”¨äº†è¿™ä¸¤ä¸ªè¿›è¡Œå¬å›

##### 3.2.1 å‘é‡å¬å›

å‘é‡å¬å›åˆ©ç”¨ FAISS è¿›è¡Œç´¢å¼•åˆ›å»ºå’ŒæŸ¥æ‰¾ï¼Œembedding åˆ©ç”¨ [M3E-large](https://modelscope.cn/models/Jerry0/M3E-large/summary) æˆ–è€…[bge-large-zh](https://modelscope.cn/models/AI-ModelScope/bge-large-zh/summary)

##### 3.2.2 bm25å¬å›

bm25å¬å›åˆ©ç”¨ langchainè‡ªå¸¦çš„bm25 retrievers

#### 3.3 é‡æ’åº

1ã€é‡æ’åºæ˜¯å¯¹å¬å›çš„æ–‡æœ¬è¿›è¡Œè¿›ä¸€æ­¥çš„é‡æ’ï¼Œä»¥è·å¾—æ›´ç²¾å‡†ï¼Œæ•°æ®é‡æ›´å°‘çš„å¯èƒ½ç­”æ¡ˆã€‚
2ã€å‘é‡å¬å›ä¸­ä½¿ç”¨çš„æ˜¯bi-encoderç»“æ„ï¼Œè€Œbge-reranker-large ä½¿ç”¨çš„æ˜¯ cross-encoderç»“æ„ï¼Œcross-encoderç»“æ„ä¸€å®šç¨‹åº¦ä¸Šè¦ä¼˜äºbi-encoder

##### 3.3.1 cross-encoder

é‡æ’åºæ­¤å¤„ä½¿ç”¨äº† [bge-reranker-large](https://modelscope.cn/models/Xorbits/bge-reranker-large/files)

#### 3.4 æ¨ç†ä¼˜åŒ–

##### 3.4.1 vllm batch

vllm åˆ©ç”¨page attention æŠ€æœ¯ä½¿æ¨ç†é€Ÿåº¦å¾—åˆ°æå‡ï¼Œbatchæ¨ç†æ¯”æ™®é€šæ¨ç†æœ‰æ¥è¿‘1å€çš„æå‡ç©ºé—´

##### 3.4.2 tensorRT-LLM

tensorRT-LLMæ˜¯è‹±ä¼Ÿè¾¾æ¨å‡ºçš„æ¨ç†æ¡†æ¶,å¹¶ä¸”æä¾›äº†c++å’Œpythonçš„è°ƒç”¨æ–¹å¼ã€‚å…³äºqwençš„tensorRT-LLMä½¿ç”¨è¯·å‚è€ƒå®˜æ–¹ä»‹ç»[tensorRT-LLM Qwen](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/qwen)

### 4ã€æ’å

[åˆèµ›2å](https://tianchi.aliyun.com/competition/entrance/532154/rankingList)
[å¤èµ›13å](https://tianchi.aliyun.com/competition/entrance/532154/rankingList)
